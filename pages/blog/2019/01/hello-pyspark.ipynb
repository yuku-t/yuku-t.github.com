{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Apatch Spark](https://spark.apache.org) は JVM 上に実装されたオープンソースの分散処理プログラミング環境で、[近年急速な盛り上がりを見せています](http://fortune.com/2015/09/25/apache-spark-survey/)。 今後 [pyspark](https://spark.apache.org/docs/latest/api/python/index.html) を通じて Spark を使う予定なので、試しに触ってみたいのですが、いかんせん JVM に慣れていないこともあって Spark のインストールから始めると大変です。\n",
    "\n",
    "そこでこの記事では Docker を使って Spark と pyspark の環境を構築します。\n",
    "\n",
    "## 環境を用意する\n",
    "\n",
    "Jupyter Lab が公開している [jupyter/pyspark-notebook](https://hub.docker.com/r/jupyter/pyspark-notebook/) というズバリな Docker イメージがあるのでこれを使います。さしあたり現時点で公開されている最新版（`87210526f381`）を指定する docker-compose.yml を用意しました:\n",
    "\n",
    "```yaml\n",
    "version: \"3\"\n",
    "services:\n",
    "  notebook:\n",
    "    image: jupyter/pyspark-notebook:87210526f381\n",
    "    working_dir: /app/notebooks\n",
    "    ports:\n",
    "      - 8888:8888\n",
    "    volumes:\n",
    "      - ./notebooks:/app/notebooks\n",
    "      - ./data:/app/data\n",
    "```\n",
    "\n",
    "2 つのディレクトリをマウントしています:\n",
    "\n",
    "- `./notebooks` - Jupyter Notebook を保存するディレクトリ。作った Notebook を Docker の外に持ち出す目的でマウントしています。\n",
    "- `./data` - 分析用のデータを格納するディレクトリです。今回の記事では使っていませんが、 CSV ファイルなどをここにおいて Jupyter から触れるようにします。\n",
    "\n",
    "用意ができたら `docker-compse up` を実行して起動します:\n",
    "\n",
    "```\n",
    "$ docker-compose up\n",
    "Starting pyspark-notebook_notebook_1 ... done\n",
    "Attaching to pyspark-notebook_notebook_1\n",
    "notebook_1  | Executing the command: jupyter notebook\n",
    "notebook_1  | [I 10:19:29.742 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab\n",
    "notebook_1  | [I 10:19:29.743 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab\n",
    "notebook_1  | [I 10:19:29.745 NotebookApp] Serving notebooks from local directory: /app/notebooks\n",
    "notebook_1  | [I 10:19:29.745 NotebookApp] The Jupyter Notebook is running at:\n",
    "notebook_1  | [I 10:19:29.745 NotebookApp] http://(cbe66ae89e09 or 127.0.0.1):8888/?token=TOKEN\n",
    "notebook_1  | [I 10:19:29.745 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n",
    "notebook_1  | [C 10:19:29.746 NotebookApp]\n",
    "notebook_1  |\n",
    "notebook_1  |     Copy/paste this URL into your browser when you connect for the first time,\n",
    "notebook_1  |     to login with a token:\n",
    "notebook_1  |         http://(HASH or 127.0.0.1):8888/?token=TOKEN\n",
    "```\n",
    "\n",
    "最後に表示された URL にアクセスすれば pyspark が使える Jupyter Notebook が表示されます。簡単ですね。インストールされている pyspark のバージョンは最新の \"2.4.0\" でした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみにこの記事はまさにこうして起動した Jupyter Notebook を使って書かれています。\n",
    "\n",
    "## Spark クラスタを起動する\n",
    "\n",
    "Spark は通常クラスタを作って分散処理を行いますが、開発段階からクラスタを作るのは大変なので [local mode](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-local.html) が用意されています。\n",
    "\n",
    "pyspark から local mode で Spark を起動します。`SparkContext` に渡している文字列は利用可能スレッド数を意味していて:\n",
    "\n",
    "- `local` - 1 スレッドのみ使う\n",
    "- `local[n]` - `n` スレッド使う（`n` は実際には数字が入る）\n",
    "- `local[*]` - JVM で使えるプロセッサーの数だけスレッドを使う（内部では [`Runtime.getRuntime.availableProcessors()`](https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#availableProcessors--) を使っているらしい）\n",
    "\n",
    "ということを表しています。一般的には `local[*]` が使われるようです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "試しに 0 から 10 までの数字の合計を計算してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使い終わったら停止します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## おわりに\n",
    "\n",
    "この記事では Docker を使って pyspark 環境を作り、実際に Spark クラスタを起動してみました。Spark のことはまだまだ全然分かりませんが少しずつできることを増やしていこうと思います。\n",
    "\n",
    "## 参考\n",
    "\n",
    "- [jupyter/pyspark-notebook - Docker Hub](https://hub.docker.com/r/jupyter/pyspark-notebook/)\n",
    "- [Image Specifics — docker-stacks latest documentation](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/specifics.html#apache-spark)\n",
    "- [pyspark package — PySpark 2.4.0 documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html)\n",
    "- [Get Started with PySpark and Jupyter Notebook in 3 Minutes](https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f)\n",
    "- [Spark local (pseudo-cluster) · Mastering Apache Spark](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-local.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
