{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Apatch Spark](https://spark.apache.org) は JVM 上に実装されたオープンソースの分散処理プログラミング環境で、[近年急速な盛り上がりを見せています](http://fortune.com/2015/09/25/apache-spark-survey/)。 今後 [pyspark](https://spark.apache.org/docs/latest/api/python/index.html) を通じて Spark を使う予定なので、試しに触ってみたいのですが、いかんせん JVM に慣れていないこともあって Spark のインストールから始めると大変です。\n",
    "\n",
    "そこでこの記事では Docker を使って Spark と pyspark の環境を構築します。\n",
    "\n",
    "## 環境を用意する\n",
    "\n",
    "Jupyter Lab が公開している [jupyter/pyspark-notebook](https://hub.docker.com/r/jupyter/pyspark-notebook/) というズバリな Docker イメージがあるのでこれを使います。さしあたり現時点で公開されている最新版（`87210526f381`）を指定する docker-compose.yml を用意しました:\n",
    "\n",
    "```yaml\n",
    "version: \"3\"\n",
    "services:\n",
    "  notebook:\n",
    "    image: jupyter/pyspark-notebook:87210526f381\n",
    "    working_dir: /app/notebooks\n",
    "    ports:\n",
    "      - 8888:8888\n",
    "    volumes:\n",
    "      - ./notebooks:/app/notebooks\n",
    "      - ./data:/app/data\n",
    "```\n",
    "\n",
    "2 つのディレクトリをマウントしています:\n",
    "\n",
    "- `./notebooks` - Jupyter Notebook を保存するディレクトリ。作った Notebook を Docker の外に持ち出す目的でマウントしています。\n",
    "- `./data` - 分析用のデータを格納するディレクトリです。今回の記事では使っていませんが、 CSV ファイルなどをここにおいて Jupyter から触れるようにします。\n",
    "\n",
    "用意ができたら `docker-compse up` を実行して起動します。すると何やらログが表示される中に URL が表示されるので、その URL にアクセスすれば pyspark が使える Jupyter Notebook が表示されます。簡単ですね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみにこの記事はまさにこうして起動した Jupyter Notebook を使って書かれています。\n",
    "\n",
    "## Spark クラスタを起動する\n",
    "\n",
    "Spark は通常クラスタを作って分散処理を行いますが、開発段階からクラスタを作るのは大変なので [local mode](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-local.html) が用意されています。\n",
    "\n",
    "pyspark から local mode で Spark を起動するには [`pyspark.SparkContext`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext) を実行します:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "渡している文字列は利用可能スレッド数を意味していて:\n",
    "\n",
    "- `local` - 1 スレッドのみ使う\n",
    "- `local[n]` - `n` スレッド使う（`n` は実際には数字が入る）\n",
    "- `local[*]` - JVM で使えるプロセッサーの数だけスレッドを使う（内部では [`Runtime.getRuntime.availableProcessors()`](https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#availableProcessors--) を使っているらしい）\n",
    "\n",
    "ということを表しています。一般的には `local[*]` が使われるようです。\n",
    "\n",
    "試しに 0 から 10 までの数字の合計を計算してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使い終わったら停止します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## おわりに\n",
    "\n",
    "この記事では Docker を使って pyspark 環境を作り、実際に Spark クラスタを起動してみました。Spark のことはまだまだ全然分かりませんが少しずつできることを増やしていこうと思います。\n",
    "\n",
    "## 参考\n",
    "\n",
    "- [jupyter/pyspark-notebook - Docker Hub](https://hub.docker.com/r/jupyter/pyspark-notebook/)\n",
    "- [Image Specifics — docker-stacks latest documentation](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/specifics.html#apache-spark)\n",
    "- [pyspark package — PySpark 2.4.0 documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html)\n",
    "- [Get Started with PySpark and Jupyter Notebook in 3 Minutes](https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f)\n",
    "- [Spark local (pseudo-cluster) · Mastering Apache Spark](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-local.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
