(window.webpackJsonp=window.webpackJsonp||[]).push([[13],{390:function(e,a,t){__NEXT_REGISTER_PAGE("/blog/2019/01/hello-pyspark",function(){return e.exports=t(410),{page:e.exports.default}})},410:function(e,a,t){"use strict";t.r(a);var n=t(3),l=t.n(n),r=t(1),c=t.n(r),o=t(5),s=function(){return c.a.createElement("div",{className:"nb-notebook"},c.a.createElement("div",{className:"nb-worksheet"},c.a.createElement("div",{className:"nb-cell nb-markdown-cell"},c.a.createElement("p",null,c.a.createElement("a",{href:"https://spark.apache.org"},"Apatch Spark")," は JVM 上に実装されたオープンソースの分散処理プログラミング環境で、",c.a.createElement("a",{href:"http://fortune.com/2015/09/25/apache-spark-survey/"},"近年急速な盛り上がりを見せています"),"。 今後 ",c.a.createElement("a",{href:"https://spark.apache.org/docs/latest/api/python/index.html"},"pyspark")," を通じて Spark を使う予定なので、試しに触ってみたいのですが、いかんせん JVM に慣れていないこともあって Spark のインストールから始めると大変です。"),c.a.createElement("p",null,"そこでこの記事では Docker を使って Spark と pyspark の環境を構築します。"),c.a.createElement("h2",null,"環境を用意する"),c.a.createElement("p",null,"Jupyter Lab が公開している ",c.a.createElement("a",{href:"https://hub.docker.com/r/jupyter/pyspark-notebook/"},"jupyter/pyspark-notebook")," というズバリな Docker イメージがあるのでこれを使います。さしあたり最新バージョンをとってきました:"),c.a.createElement("pre",{className:"language-bash"},c.a.createElement("code",{className:"language-bash"},"docker pull jupyter/pyspark-notebook:87210526f381","\n")),c.a.createElement("p",null,"起動します:"),c.a.createElement("pre",{className:"language-bash"},c.a.createElement("code",{className:"language-bash"},"docker run --rm -w /app -p ",c.a.createElement("span",{className:"token number"},"8888"),":8888 ",c.a.createElement("span",{className:"token punctuation"},"\\"),"\n","    ","--mount ",c.a.createElement("span",{className:"token assign-left variable"},"type"),c.a.createElement("span",{className:"token operator"},"="),"bind,src",c.a.createElement("span",{className:"token operator"},"="),c.a.createElement("span",{className:"token variable"},c.a.createElement("span",{className:"token variable"},"$("),c.a.createElement("span",{className:"token builtin class-name"},"pwd"),c.a.createElement("span",{className:"token variable"},")")),",dst",c.a.createElement("span",{className:"token operator"},"="),"/app ",c.a.createElement("span",{className:"token punctuation"},"\\"),"\n","    ","jupyter/pyspark-notebook:87210526f381","\n")),c.a.createElement("p",null,"すると何やらログが表示される中に URL が表示されるので、その URL にアクセスすれば pyspark が使える Jupyter Notebook が表示されます。簡単ですね。")),c.a.createElement("div",{className:"nb-cell nb-code-cell"},c.a.createElement("div",{className:"nb-input","data-prompt-number":1},c.a.createElement("pre",{className:"language-python"},c.a.createElement("code",{className:"language-python","data-language":"python"},c.a.createElement("span",{className:"token keyword"},"import")," pyspark","\n","pyspark",c.a.createElement("span",{className:"token punctuation"},"."),"version",c.a.createElement("span",{className:"token punctuation"},"."),"__version__"))),c.a.createElement("div",{className:"nb-output","data-prompt-number":1},c.a.createElement("pre",{className:"nb-text-output"},"'2.4.0'"))),c.a.createElement("div",{className:"nb-cell nb-markdown-cell"},c.a.createElement("p",null,"ちなみにこの記事はまさにこうして起動した Jupyter Notebook を使って書かれています。"),c.a.createElement("h2",null,"Spark クラスタを起動する"),c.a.createElement("p",null,"Spark は通常クラスタを作って分散処理を行いますが、開発段階からクラスタを作るのは大変なので ",c.a.createElement("a",{href:"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-local.html"},"local mode")," が用意されています。"),c.a.createElement("p",null,"pyspark から local mode で Spark を起動するには ",c.a.createElement("a",{href:"https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext"},c.a.createElement("code",null,"pyspark.SparkContext"))," を実行します:")),c.a.createElement("div",{className:"nb-cell nb-code-cell"},c.a.createElement("div",{className:"nb-input","data-prompt-number":2},c.a.createElement("pre",{className:"language-python"},c.a.createElement("code",{className:"language-python","data-language":"python"},"sc ",c.a.createElement("span",{className:"token operator"},"=")," pyspark",c.a.createElement("span",{className:"token punctuation"},"."),"SparkContext",c.a.createElement("span",{className:"token punctuation"},"("),c.a.createElement("span",{className:"token string"},"'local[*]'"),c.a.createElement("span",{className:"token punctuation"},")"))))),c.a.createElement("div",{className:"nb-cell nb-markdown-cell"},c.a.createElement("p",null,"渡している文字列は利用可能スレッド数を意味していて:"),c.a.createElement("ul",null,c.a.createElement("li",null,c.a.createElement("code",null,"local")," - 1 スレッドのみ使う"),c.a.createElement("li",null,c.a.createElement("code",null,"local[n]")," - ",c.a.createElement("code",null,"n")," スレッド使う（",c.a.createElement("code",null,"n")," は実際には数字が入る）"),c.a.createElement("li",null,c.a.createElement("code",null,"local[*]")," - JVM で使えるプロセッサーの数だけスレッドを使う（内部では ",c.a.createElement("a",{href:"https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#availableProcessors--"},c.a.createElement("code",null,"Runtime.getRuntime.availableProcessors()"))," を使っているらしい）")),c.a.createElement("p",null,"ということを表しています。一般的には ",c.a.createElement("code",null,"local[*]")," が使われるようです。"),c.a.createElement("p",null,"試しに 0 から 10 までの数字の合計を計算してみます。")),c.a.createElement("div",{className:"nb-cell nb-code-cell"},c.a.createElement("div",{className:"nb-input","data-prompt-number":3},c.a.createElement("pre",{className:"language-python"},c.a.createElement("code",{className:"language-python","data-language":"python"},"rdd ",c.a.createElement("span",{className:"token operator"},"=")," sc",c.a.createElement("span",{className:"token punctuation"},"."),"parallelize",c.a.createElement("span",{className:"token punctuation"},"("),c.a.createElement("span",{className:"token builtin"},"range"),c.a.createElement("span",{className:"token punctuation"},"("),c.a.createElement("span",{className:"token number"},"10"),c.a.createElement("span",{className:"token punctuation"},")"),c.a.createElement("span",{className:"token punctuation"},")"),"\n","rdd",c.a.createElement("span",{className:"token punctuation"},"."),c.a.createElement("span",{className:"token builtin"},"sum"),c.a.createElement("span",{className:"token punctuation"},"("),c.a.createElement("span",{className:"token punctuation"},")")))),c.a.createElement("div",{className:"nb-output","data-prompt-number":3},c.a.createElement("pre",{className:"nb-text-output"},"45"))),c.a.createElement("div",{className:"nb-cell nb-markdown-cell"},c.a.createElement("p",null,"使い終わったら停止します。")),c.a.createElement("div",{className:"nb-cell nb-code-cell"},c.a.createElement("div",{className:"nb-input","data-prompt-number":4},c.a.createElement("pre",{className:"language-python"},c.a.createElement("code",{className:"language-python","data-language":"python"},"sc",c.a.createElement("span",{className:"token punctuation"},"."),"stop",c.a.createElement("span",{className:"token punctuation"},"("),c.a.createElement("span",{className:"token punctuation"},")"))))),c.a.createElement("div",{className:"nb-cell nb-markdown-cell"},c.a.createElement("h2",null,"おわりに"),c.a.createElement("p",null,"この記事では Docker を使って pyspark 環境を作り、実際に Spark クラスタを起動してみました。Spark のことはまだまだ全然分かりませんが少しずつできることを増やしていこうと思います。"),c.a.createElement("h2",null,"参考"),c.a.createElement("ul",null,c.a.createElement("li",null,c.a.createElement("a",{href:"https://hub.docker.com/r/jupyter/pyspark-notebook/"},"jupyter/pyspark-notebook - Docker Hub")),c.a.createElement("li",null,c.a.createElement("a",{href:"https://jupyter-docker-stacks.readthedocs.io/en/latest/using/specifics.html#apache-spark"},"Image Specifics — docker-stacks latest documentation")),c.a.createElement("li",null,c.a.createElement("a",{href:"https://spark.apache.org/docs/latest/api/python/pyspark.html"},"pyspark package — PySpark 2.4.0 documentation")),c.a.createElement("li",null,c.a.createElement("a",{href:"https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f"},"Get Started with PySpark and Jupyter Notebook in 3 Minutes")),c.a.createElement("li",null,c.a.createElement("a",{href:"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-local.html"},"Spark local (pseudo-cluster) · Mastering Apache Spark"))))))};function p(e){return(p="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e})(e)}function m(e,a,t,n,l,r,c){try{var o=e[r](c),s=o.value}catch(e){return void t(e)}o.done?a(s):Promise.resolve(s).then(n,l)}function u(e,a){for(var t=0;t<a.length;t++){var n=a[t];n.enumerable=n.enumerable||!1,n.configurable=!0,"value"in n&&(n.writable=!0),Object.defineProperty(e,n.key,n)}}function i(e,a){return!a||"object"!==p(a)&&"function"!=typeof a?function(e){if(void 0===e)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return e}(e):a}function k(e){return(k=Object.setPrototypeOf?Object.getPrototypeOf:function(e){return e.__proto__||Object.getPrototypeOf(e)})(e)}function E(e,a){return(E=Object.setPrototypeOf||function(e,a){return e.__proto__=a,e})(e,a)}t.d(a,"default",function(){return b});var b=function(e){function a(){return function(e,a){if(!(e instanceof a))throw new TypeError("Cannot call a class as a function")}(this,a),i(this,k(a).apply(this,arguments))}var n,r,p;return function(e,a){if("function"!=typeof a&&null!==a)throw new TypeError("Super expression must either be null or a function");e.prototype=Object.create(a&&a.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),a&&E(e,a)}(a,c.a.Component),n=a,r=[{key:"render",value:function(){return c.a.createElement(o.b,{meta:this.props.meta},c.a.createElement(s,null))}}],p=[{key:"getInitialProps",value:function(){var e,a=(e=l.a.mark(function e(){var a,n;return l.a.wrap(function(e){for(;;)switch(e.prev=e.next){case 0:return a=t(12),n=a.entries,e.abrupt("return",{meta:n["blog/2019/01/hello-pyspark"]});case 2:case"end":return e.stop()}},e)}),function(){var a=this,t=arguments;return new Promise(function(n,l){var r=e.apply(a,t);function c(e){m(r,n,l,c,o,"next",e)}function o(e){m(r,n,l,c,o,"throw",e)}c(void 0)})});return function(){return a.apply(this,arguments)}}()}],r&&u(n.prototype,r),p&&u(n,p),a}()}},[[390,1,0,2]]]);